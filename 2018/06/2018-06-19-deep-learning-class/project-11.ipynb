{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chyld/.local/apps/miniconda3/envs/gamma/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, SpatialDropout1D\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 5000\n",
    "maxlen = 80\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of reviews\n",
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 218\n",
      "1 189\n",
      "2 141\n",
      "3 550\n",
      "4 147\n",
      "5 43\n",
      "6 123\n",
      "7 562\n",
      "8 233\n",
      "9 130\n"
     ]
    }
   ],
   "source": [
    "# words in each review\n",
    "for i in range(10):\n",
    "    print(i, len(x_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentiment, positive or negative\n",
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_FROM = -5000\n",
    "word_to_id = imdb.get_word_index()\n",
    "id_to_word = {v:k for k,v in word_to_id.items()}\n",
    "\n",
    "# word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
    "# word_to_id[\"<PAD>\"] = 0\n",
    "# word_to_id[\"<START>\"] = 1\n",
    "# word_to_id[\"<UNK>\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_id['this']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_word[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "this\n",
      "film\n",
      "was\n",
      "just\n",
      "brilliant\n",
      "casting\n",
      "location\n",
      "scenery\n",
      "story\n",
      "direction\n",
      "everyone's\n",
      "really\n",
      "suited\n",
      "the\n",
      "part\n",
      "they\n",
      "played\n",
      "and\n",
      "you\n",
      "could\n",
      "just\n",
      "imagine\n",
      "being\n",
      "there\n",
      "robert\n",
      "the\n",
      "is\n",
      "an\n",
      "amazing\n",
      "actor\n",
      "and\n",
      "now\n",
      "the\n",
      "same\n",
      "being\n",
      "director\n",
      "the\n",
      "father\n",
      "came\n",
      "from\n",
      "the\n",
      "same\n",
      "scottish\n",
      "island\n",
      "as\n",
      "myself\n",
      "so\n",
      "i\n",
      "loved\n",
      "the\n",
      "fact\n",
      "there\n",
      "was\n",
      "a\n",
      "real\n",
      "connection\n",
      "with\n",
      "this\n",
      "film\n",
      "the\n",
      "witty\n",
      "remarks\n",
      "throughout\n",
      "the\n",
      "film\n",
      "were\n",
      "great\n",
      "it\n",
      "was\n",
      "just\n",
      "brilliant\n",
      "so\n",
      "much\n",
      "that\n",
      "i\n",
      "bought\n",
      "the\n",
      "film\n",
      "as\n",
      "soon\n",
      "as\n",
      "it\n",
      "was\n",
      "released\n",
      "for\n",
      "the\n",
      "and\n",
      "would\n",
      "recommend\n",
      "it\n",
      "to\n",
      "everyone\n",
      "to\n",
      "watch\n",
      "and\n",
      "the\n",
      "fly\n",
      "the\n",
      "was\n",
      "amazing\n",
      "really\n",
      "cried\n",
      "at\n",
      "the\n",
      "end\n",
      "it\n",
      "was\n",
      "so\n",
      "sad\n",
      "and\n",
      "you\n",
      "know\n",
      "what\n",
      "they\n",
      "say\n",
      "if\n",
      "you\n",
      "cry\n",
      "at\n",
      "a\n",
      "film\n",
      "it\n",
      "must\n",
      "have\n",
      "been\n",
      "good\n",
      "and\n",
      "this\n",
      "definitely\n",
      "was\n",
      "also\n",
      "the\n",
      "to\n",
      "the\n",
      "two\n",
      "little\n",
      "the\n",
      "that\n",
      "played\n",
      "the\n",
      "the\n",
      "of\n",
      "norman\n",
      "and\n",
      "paul\n",
      "they\n",
      "were\n",
      "just\n",
      "brilliant\n",
      "children\n",
      "are\n",
      "often\n",
      "left\n",
      "out\n",
      "of\n",
      "the\n",
      "the\n",
      "list\n",
      "i\n",
      "think\n",
      "because\n",
      "the\n",
      "stars\n",
      "that\n",
      "play\n",
      "them\n",
      "all\n",
      "grown\n",
      "up\n",
      "are\n",
      "such\n",
      "a\n",
      "big\n",
      "the\n",
      "for\n",
      "the\n",
      "whole\n",
      "film\n",
      "but\n",
      "these\n",
      "children\n",
      "are\n",
      "amazing\n",
      "and\n",
      "should\n",
      "be\n",
      "the\n",
      "for\n",
      "what\n",
      "they\n",
      "have\n",
      "done\n",
      "don't\n",
      "you\n",
      "think\n",
      "the\n",
      "whole\n",
      "story\n",
      "was\n",
      "so\n",
      "lovely\n",
      "because\n",
      "it\n",
      "was\n",
      "true\n",
      "and\n",
      "was\n",
      "someone's\n",
      "life\n",
      "after\n",
      "all\n",
      "that\n",
      "was\n",
      "the\n",
      "with\n",
      "us\n",
      "all\n"
     ]
    }
   ],
   "source": [
    "for idx in x_train[0]:\n",
    "    val = idx - 3\n",
    "    if val < 0:\n",
    "        val = 1\n",
    "    print(id_to_word[val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word = {value:key for key,value in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<START> whoopi correctly heights retired hyped bridges crippled sale merits additional surpasses directions denver depicting furious integrity remakes kissing wholly marion retired pete grabs sid height <UNK> pin peters skits breed kissing tube depicting ordered grabs milk <UNK> charged cancer yard depicting ordered vengeful 1994 spock bunny biography swim option depicting premiere sid heights tooth inability excruciating abraham whoopi correctly depicting seductive ing iv depicting correctly metaphor eugene assumed heights retired hyped biography loyalty domino swim homosexuality depicting correctly spock beware spock assumed heights demonstrates inevitably <UNK> kissing 1971 fantasies assumed kubrick posters kubrick dee kissing depicting denouement <UNK> heights skits directions stripper valley depicting disliked assumed heights biography nicolas kissing wholly confrontation compassion integrity \\x85 meryl wholly lester valley tooth correctly assumed stumbled othello deaf dreck kissing whoopi deniro heights checked <UNK> kubrick depicting emperor timon <UNK> domino remakes depicting <UNK> shed developments kissing duck integrity metaphor retired hyped pages underlying prepare aspiring maniac shed depicting <UNK> cookie swim technicolor significance depicting firm domino ustinov alec teaching alot assassin underlying expedition tooth warriors <UNK> inevitably depicting lundgren correctly businessman wretched pages underlying skits kissing conscious package <UNK> inevitably compassion integrity othello addicted indulgent wholly technicolor depicting lundgren merits heights biography dominated significance assumed heights wang kissing heights joshua exchange fought teaching domino heights <UNK> abraham tastes teaching'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([id_to_word[idx] for idx in x_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_word[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make all rows the same length\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 80)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 4)           80000     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 4)                 144       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 80,149\n",
      "Trainable params: 80,149\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 4))\n",
    "# model.add(SpatialDropout1D(0.5))\n",
    "model.add(LSTM(4, dropout=0.5, recurrent_dropout=0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 38s 2ms/step - loss: 0.6232 - acc: 0.6595 - val_loss: 0.5444 - val_acc: 0.7502\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 4s 180us/step\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5444021554374695, 0.75024)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No such layer: embedding_2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-4a86d44f8f6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'embedding_2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/apps/miniconda3/envs/gamma/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mget_layer\u001b[0;34m(self, name, index)\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/apps/miniconda3/envs/gamma/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mget_layer\u001b[0;34m(self, name, index)\u001b[0m\n\u001b[1;32m   1889\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No such layer: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No such layer: embedding_2"
     ]
    }
   ],
   "source": [
    "embedding = model.get_layer('embedding_2').get_weights()\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding[0].shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"Word embeddings\" are a family of natural language processing techniques aiming at mapping semantic meaning into a geometric space. This is done by associating a numeric vector to every word in a dictionary, such that the distance (e.g. L2 distance or more commonly cosine distance) between any two vectors would capture part of the semantic relationship between the two associated words. The geometric space formed by these vectors is called an embedding space.\n",
    "\n",
    "For instance, \"coconut\" and \"polar bear\" are words that are semantically quite different, so a reasonable embedding space would represent them as vectors that would be very far apart. But \"kitchen\" and \"dinner\" are related words, so they should be embedded close to each other.\n",
    "\n",
    "The most common application of this layer is for text processing. Let's see a simple example. Our training set consists only of two phrases:\n",
    "\n",
    "Hope to see you soon\n",
    "Nice to see you again\n",
    "\n",
    "So we can encode these phrases by assigning each word a unique integer number (by order of appearance in our training dataset for example). Then our phrases could be rewritten as:\n",
    "\n",
    "[0, 1, 2, 3, 4]\n",
    "[5, 1, 2, 3, 6]\n",
    "\n",
    "Now imagine we want to train a network whose first layer is an embeding layer. In this case, we should initialize it as follows:\n",
    "\n",
    "Embedding(7, 2, input_length=5)\n",
    "\n",
    "The first argument (7) is the number of distinct words in the training set. The second argument (2) indicates the size of the embedding vectors. The input_length argumet, of course, determines the size of each input sequence.\n",
    "\n",
    "Once the network has been trained, we can get the weights of the embedding layer, which in this case will be of size (7, 2) and can be thought as the table used to map integers to embedding vectors:\n",
    "\n",
    "+------------+------------+\n",
    "|   index    |  Embedding |\n",
    "+------------+------------+\n",
    "|     0      | [1.2, 3.1] |\n",
    "|     1      | [0.1, 4.2] |\n",
    "|     2      | [1.0, 3.1] |\n",
    "|     3      | [0.3, 2.1] |\n",
    "|     4      | [2.2, 1.4] |\n",
    "|     5      | [0.7, 1.7] |\n",
    "|     6      | [4.1, 2.0] |\n",
    "+------------+------------+\n",
    "\n",
    "So according to these embeddings, our second training phrase will be represented as:\n",
    "\n",
    "[[0.7, 1.7], [0.1, 4.2], [1.0, 3.1], [0.3, 2.1], [4.1, 2.0]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Word embeddings provide a dense representation of words and their relative meanings.\n",
    "\n",
    "They are an improvement over sparse representations used in simpler bag of word model representations.\n",
    "\n",
    "Word embeddings can be learned from text data and reused among projects. They can also be learned as part of fitting a neural network on text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict sentiment from reviews\n",
    "bad = \"this movie was terrible and bad\"\n",
    "good = \"i really liked the movie and had fun\"\n",
    "for review in [good,bad]:\n",
    "    tmp = []\n",
    "    for word in review.split(\" \"):\n",
    "        tmp.append(word_to_id[word])\n",
    "    tmp_padded = sequence.pad_sequences([tmp], maxlen=max_review_length) \n",
    "    print(\"%s. Sentiment: %s\" % (review,model.predict(array([tmp_padded][0]))[0][0]))\n",
    "i really liked the movie and had fun. Sentiment: 0.715537\n",
    "this movie was terrible and bad. Sentiment: 0.0353295"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
